{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25908d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>hotel</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Awesome visit ]</td>\n",
       "      <td>[Went on a girls trip this past weekend. We ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Super hotel and Super Staff]</td>\n",
       "      <td>[We were very lucky to win 4 nights accomodati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Roaring Fork]</td>\n",
       "      <td>[Wait until the last minute]</td>\n",
       "      <td>[Wait until the last minute and get a lodging/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Great Hotel. Nice place to stay]</td>\n",
       "      <td>[Great hotel. Beautiful. Great well decorated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[Aspen Mountain Lodge]</td>\n",
       "      <td>[Quaint and cozy lodge]</td>\n",
       "      <td>[Great value, decent location. I'd highly reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great stay]</td>\n",
       "      <td>[Spent a couple of nights in Aspen on a girls ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>4</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Excellent cost/benefit]</td>\n",
       "      <td>[We stayed in a two bedrooms/bathrooms apartme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great WInter Vaca]</td>\n",
       "      <td>[A wonderful place to stay for our family vaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Chateau Blanc for a week]</td>\n",
       "      <td>[The lodge is few blocks away from the main do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>5</td>\n",
       "      <td>[Shadow Mountain Lodge]</td>\n",
       "      <td>[Excellent Value]</td>\n",
       "      <td>[Great location, nice rooms much larger than a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                    hotel                              title  \\\n",
       "0         5        [Limelight Hotel]                   [Awesome visit ]   \n",
       "1         5        [Limelight Hotel]      [Super hotel and Super Staff]   \n",
       "2         5   [Chateau Roaring Fork]       [Wait until the last minute]   \n",
       "3         5        [Limelight Hotel]  [Great Hotel. Nice place to stay]   \n",
       "4         3   [Aspen Mountain Lodge]            [Quaint and cozy lodge]   \n",
       "...     ...                      ...                                ...   \n",
       "2001      5          [Chateau Blanc]                       [Great stay]   \n",
       "2002      4          [Chateau Blanc]           [Excellent cost/benefit]   \n",
       "2003      5          [Chateau Blanc]                [Great WInter Vaca]   \n",
       "2004      5          [Chateau Blanc]         [Chateau Blanc for a week]   \n",
       "2005      5  [Shadow Mountain Lodge]                  [Excellent Value]   \n",
       "\n",
       "                                                content  \n",
       "0     [Went on a girls trip this past weekend. We ha...  \n",
       "1     [We were very lucky to win 4 nights accomodati...  \n",
       "2     [Wait until the last minute and get a lodging/...  \n",
       "3     [Great hotel. Beautiful. Great well decorated ...  \n",
       "4     [Great value, decent location. I'd highly reco...  \n",
       "...                                                 ...  \n",
       "2001  [Spent a couple of nights in Aspen on a girls ...  \n",
       "2002  [We stayed in a two bedrooms/bathrooms apartme...  \n",
       "2003  [A wonderful place to stay for our family vaca...  \n",
       "2004  [The lodge is few blocks away from the main do...  \n",
       "2005  [Great location, nice rooms much larger than a...  \n",
       "\n",
       "[2006 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('aspen.json')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81cc4c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>hotel</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Awesome visit ]</td>\n",
       "      <td>[Went on a girls trip this past weekend. We ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Super hotel and Super Staff]</td>\n",
       "      <td>[We were very lucky to win 4 nights accomodati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Roaring Fork]</td>\n",
       "      <td>[Wait until the last minute]</td>\n",
       "      <td>[Wait until the last minute and get a lodging/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Great Hotel. Nice place to stay]</td>\n",
       "      <td>[Great hotel. Beautiful. Great well decorated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[Aspen Mountain Lodge]</td>\n",
       "      <td>[Quaint and cozy lodge]</td>\n",
       "      <td>[Great value, decent location. I'd highly reco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great stay]</td>\n",
       "      <td>[Spent a couple of nights in Aspen on a girls ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>4</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Excellent cost/benefit]</td>\n",
       "      <td>[We stayed in a two bedrooms/bathrooms apartme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great WInter Vaca]</td>\n",
       "      <td>[A wonderful place to stay for our family vaca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Chateau Blanc for a week]</td>\n",
       "      <td>[The lodge is few blocks away from the main do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>5</td>\n",
       "      <td>[Shadow Mountain Lodge]</td>\n",
       "      <td>[Excellent Value]</td>\n",
       "      <td>[Great location, nice rooms much larger than a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                    hotel                              title  \\\n",
       "0         5        [Limelight Hotel]                   [Awesome visit ]   \n",
       "1         5        [Limelight Hotel]      [Super hotel and Super Staff]   \n",
       "2         5   [Chateau Roaring Fork]       [Wait until the last minute]   \n",
       "3         5        [Limelight Hotel]  [Great Hotel. Nice place to stay]   \n",
       "4         3   [Aspen Mountain Lodge]            [Quaint and cozy lodge]   \n",
       "...     ...                      ...                                ...   \n",
       "2001      5          [Chateau Blanc]                       [Great stay]   \n",
       "2002      4          [Chateau Blanc]           [Excellent cost/benefit]   \n",
       "2003      5          [Chateau Blanc]                [Great WInter Vaca]   \n",
       "2004      5          [Chateau Blanc]         [Chateau Blanc for a week]   \n",
       "2005      5  [Shadow Mountain Lodge]                  [Excellent Value]   \n",
       "\n",
       "                                                content  rating  \n",
       "0     [Went on a girls trip this past weekend. We ha...       1  \n",
       "1     [We were very lucky to win 4 nights accomodati...       1  \n",
       "2     [Wait until the last minute and get a lodging/...       1  \n",
       "3     [Great hotel. Beautiful. Great well decorated ...       1  \n",
       "4     [Great value, decent location. I'd highly reco...       0  \n",
       "...                                                 ...     ...  \n",
       "2001  [Spent a couple of nights in Aspen on a girls ...       1  \n",
       "2002  [We stayed in a two bedrooms/bathrooms apartme...       0  \n",
       "2003  [A wonderful place to stay for our family vaca...       1  \n",
       "2004  [The lodge is few blocks away from the main do...       1  \n",
       "2005  [Great location, nice rooms much larger than a...       1  \n",
       "\n",
       "[2006 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the DataFrame df with columns 'stars', 'hotel', 'title', and 'content'\n",
    "\n",
    "# Define a function to categorize stars\n",
    "def categorize_stars(stars):\n",
    "    if stars in [ 5]:\n",
    "        return 1\n",
    "    elif stars in [1, 2,3,4]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Read your DataFrame from the provided data\n",
    "# df = pd.read_csv('your_data.csv')  # Uncomment and replace 'your_data.csv' with your file path if you're reading from a CSV file\n",
    "\n",
    "# Apply the categorize_stars function to the 'stars' column and create a new column 'rating'\n",
    "df['rating'] = df['stars'].apply(categorize_stars)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20568ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#pre-processing of text\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4d3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(str_list, lemmatize=True):\n",
    "    clean_list = []\n",
    "    \n",
    "    for text in str_list:\n",
    "        # Remove pound sign from hashtags\n",
    "        text = re.sub(r'#', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        clean_words = []\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()  # Move lemmatizer initialization outside the loop\n",
    "        \n",
    "        for word in words:\n",
    "            # Drop words with fewer than 2 characters and drop any punctuation \"words\"\n",
    "            if len(word) > 1 and re.match(r'^\\w+$', word):\n",
    "                if lemmatize:\n",
    "                    word = lemmatizer.lemmatize(word)  # Apply lemmatization\n",
    "                clean_words.append(word)\n",
    "        \n",
    "        clean_text = ' '.join(clean_words)\n",
    "        clean_list.append(clean_text)\n",
    "    \n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cedc5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>hotel</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Awesome visit ]</td>\n",
       "      <td>['Went on a girls trip this past weekend. We h...</td>\n",
       "      <td>1</td>\n",
       "      <td>on girl trip this past weekend we had wonderfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Super hotel and Super Staff]</td>\n",
       "      <td>['We were very lucky to win 4 nights accomodat...</td>\n",
       "      <td>1</td>\n",
       "      <td>were very lucky to win night accomodation at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Roaring Fork]</td>\n",
       "      <td>[Wait until the last minute]</td>\n",
       "      <td>[\"Wait until the last minute and get a lodging...</td>\n",
       "      <td>1</td>\n",
       "      <td>wait until the last minute and get ticket pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[Limelight Hotel]</td>\n",
       "      <td>[Great Hotel. Nice place to stay]</td>\n",
       "      <td>['Great hotel. Beautiful. Great well decorated...</td>\n",
       "      <td>1</td>\n",
       "      <td>hotel beautiful great well decorated bar and n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[Aspen Mountain Lodge]</td>\n",
       "      <td>[Quaint and cozy lodge]</td>\n",
       "      <td>[\"Great value, decent location. I'd highly rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>great value decent location highly recommend r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great stay]</td>\n",
       "      <td>['Spent a couple of nights in Aspen on a girls...</td>\n",
       "      <td>1</td>\n",
       "      <td>couple of night in aspen on girl getaway our t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>4</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Excellent cost/benefit]</td>\n",
       "      <td>['We stayed in a two bedrooms/bathrooms apartm...</td>\n",
       "      <td>0</td>\n",
       "      <td>stayed in two apartment the apartment had pret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Great WInter Vaca]</td>\n",
       "      <td>['A wonderful place to stay for our family vac...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful place to stay for our family vacatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>5</td>\n",
       "      <td>[Chateau Blanc]</td>\n",
       "      <td>[Chateau Blanc for a week]</td>\n",
       "      <td>[\"The lodge is few blocks away from the main d...</td>\n",
       "      <td>1</td>\n",
       "      <td>the lodge is few block away from the main down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>5</td>\n",
       "      <td>[Shadow Mountain Lodge]</td>\n",
       "      <td>[Excellent Value]</td>\n",
       "      <td>['Great location, nice rooms much larger than ...</td>\n",
       "      <td>1</td>\n",
       "      <td>location nice room much larger than hotel room...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                    hotel                              title  \\\n",
       "0         5        [Limelight Hotel]                   [Awesome visit ]   \n",
       "1         5        [Limelight Hotel]      [Super hotel and Super Staff]   \n",
       "2         5   [Chateau Roaring Fork]       [Wait until the last minute]   \n",
       "3         5        [Limelight Hotel]  [Great Hotel. Nice place to stay]   \n",
       "4         3   [Aspen Mountain Lodge]            [Quaint and cozy lodge]   \n",
       "...     ...                      ...                                ...   \n",
       "2001      5          [Chateau Blanc]                       [Great stay]   \n",
       "2002      4          [Chateau Blanc]           [Excellent cost/benefit]   \n",
       "2003      5          [Chateau Blanc]                [Great WInter Vaca]   \n",
       "2004      5          [Chateau Blanc]         [Chateau Blanc for a week]   \n",
       "2005      5  [Shadow Mountain Lodge]                  [Excellent Value]   \n",
       "\n",
       "                                                content  rating  \\\n",
       "0     ['Went on a girls trip this past weekend. We h...       1   \n",
       "1     ['We were very lucky to win 4 nights accomodat...       1   \n",
       "2     [\"Wait until the last minute and get a lodging...       1   \n",
       "3     ['Great hotel. Beautiful. Great well decorated...       1   \n",
       "4     [\"Great value, decent location. I'd highly rec...       0   \n",
       "...                                                 ...     ...   \n",
       "2001  ['Spent a couple of nights in Aspen on a girls...       1   \n",
       "2002  ['We stayed in a two bedrooms/bathrooms apartm...       0   \n",
       "2003  ['A wonderful place to stay for our family vac...       1   \n",
       "2004  [\"The lodge is few blocks away from the main d...       1   \n",
       "2005  ['Great location, nice rooms much larger than ...       1   \n",
       "\n",
       "                                             clean_text  \n",
       "0     on girl trip this past weekend we had wonderfu...  \n",
       "1     were very lucky to win night accomodation at t...  \n",
       "2     wait until the last minute and get ticket pack...  \n",
       "3     hotel beautiful great well decorated bar and n...  \n",
       "4     great value decent location highly recommend r...  \n",
       "...                                                 ...  \n",
       "2001  couple of night in aspen on girl getaway our t...  \n",
       "2002  stayed in two apartment the apartment had pret...  \n",
       "2003  wonderful place to stay for our family vacatio...  \n",
       "2004  the lodge is few block away from the main down...  \n",
       "2005  location nice room much larger than hotel room...  \n",
       "\n",
       "[2006 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df['content'].astype(str)\n",
    "df['clean_text']= clean_text(df['content'])\n",
    "df['clean_text']= df['clean_text'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b98a99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "5    0.651545\n",
       "4    0.222333\n",
       "3    0.063310\n",
       "1    0.034397\n",
       "2    0.028415\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85f9c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hotel\n",
       "[Mountain Chalet Aspen]                              0.089731\n",
       "[Aspen Square Condominium Hotel]                     0.082253\n",
       "[The St. Regis Aspen Resort]                         0.082253\n",
       "[Hotel Jerome]                                       0.082253\n",
       "[Tyrolean Lodge]                                     0.080758\n",
       "[Molly Gibson Lodge]                                 0.072283\n",
       "[Aspen Mountain Lodge]                               0.039880\n",
       "[Hotel Durant]                                       0.039880\n",
       "[The Little Nell]                                    0.029910\n",
       "[Lift One Condominiums]                              0.029910\n",
       "[Snow Queen Lodge]                                   0.029412\n",
       "[Residences at The Little Nell]                      0.027916\n",
       "[Aspen Alps Condominium Resort]                      0.027418\n",
       "[The Residence Hotel]                                0.022433\n",
       "[Hyatt Residence Club Grand Aspen]                   0.022433\n",
       "[Difficult Campground]                               0.021436\n",
       "[Annabelle Inn]                                      0.019940\n",
       "[Limelight Hotel]                                    0.019940\n",
       "[The Innsbruck]                                      0.017448\n",
       "[The Inn at Aspen]                                   0.017448\n",
       "[Hearthstone House]                                  0.017448\n",
       "[Aspen Meadows Resort]                               0.014955\n",
       "[The Prospector Condominiums]                        0.014955\n",
       "[Chateau Blanc]                                      0.012463\n",
       "[The Ritz-Carlton Club, Aspen Highlands]             0.009970\n",
       "[Shadow Mountain Lodge]                              0.008475\n",
       "[St. Moritz Lodge & Condominiums]                    0.007478\n",
       "[The Independence Square]                            0.007478\n",
       "[Hotel Aspen]                                        0.007478\n",
       "[10th Mountain Division Hut Association]             0.007478\n",
       "[Chateau Roaring Fork]                               0.005982\n",
       "[Chalet Lisl]                                        0.004985\n",
       "[North of Nell]                                      0.004985\n",
       "[Chateau Aspen Condominiums]                         0.003988\n",
       "[The Gant]                                           0.002493\n",
       "[Shadow Mountain Condominiums]                       0.001994\n",
       "[Grand Aspen]                                        0.001496\n",
       "[Aspen Townhouse Rental]                             0.001496\n",
       "[ResortQuest Chateaux Dumont & Chaumont]             0.001496\n",
       "[Lost Man Campground]                                0.000997\n",
       "[Sardy House Residence and Carriage House Inn]       0.000997\n",
       "[Alpenblick]                                         0.000997\n",
       "[Chateau Chaumont and Dumont by Frias Properties]    0.000997\n",
       "[Downtown Aspen by Gondola Resorts]                  0.000997\n",
       "[Beaumont Inn]                                       0.000499\n",
       "[Silver Shadow Aspen/Snowmass]                       0.000499\n",
       "[Heatherbed Mountain Lodge]                          0.000499\n",
       "[Skiers Chalet]                                      0.000499\n",
       "[Concept 600 Condominiums]                           0.000499\n",
       "[210 Cooper Condominimums]                           0.000499\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hotel'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e2ad713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3051ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1    0.651545\n",
       "0    0.348455\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['rating'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9910c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['clean_text'], df['rating'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['rating'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec08fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7563be4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4c3d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14ae0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train ['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train ['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val ['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val ['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test ['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test ['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a7957e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a35d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6d991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e79e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Pass the pre-trained BERT to our defined architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# Push the model to CPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accc7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.43558282 0.76721311]\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
    "\n",
    "#The compute_class_weight function from the sklearn.utils.class_weight module \n",
    "#is used to compute the class weights with multiple parameters for the training labels.\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute the class weights\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "294e1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac79c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8777194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aa4af5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.695\n",
      "Validation Loss: 0.692\n",
      "\n",
      " Epoch 2 / 3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.695\n",
      "Validation Loss: 0.690\n",
      "\n",
      " Epoch 3 / 3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.689\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "#defining epochs\n",
    "epochs = 3\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c55cdbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.12      0.18       105\n",
      "           1       0.65      0.86      0.74       196\n",
      "\n",
      "    accuracy                           0.60       301\n",
      "   macro avg       0.49      0.49      0.46       301\n",
      "weighted avg       0.54      0.60      0.54       301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba12946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
